{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "M20MDQcNg7XA"
      ],
      "authorship_tag": "ABX9TyMiP7KjGS73ZWfm8EeONaFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desve/labelcraft-2025-ml-challenge/blob/main/03_baseline_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "03 — Baseline Transformer (LabelCraft 2025)\n",
        "В этом ноутбуке:\n",
        "\n",
        "* используем labeled_train.parquet и category_tree.csv;\n",
        "\n",
        "* готовим поле text_clean;\n",
        "\n",
        "* обучаем простой классификатор на базе RuBERT/ruRoBERTa;\n",
        "\n",
        "* считаем macro/micro F1 на валидации и сравниваем с TF-IDF baseline."
      ],
      "metadata": {
        "id": "QT9p8Py_guzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорты, монтирование Drive, пути"
      ],
      "metadata": {
        "id": "M20MDQcNg7XA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "a7HxGw7Fg2H0",
        "outputId": "a6d2d16b-e685-497c-944a-b6e1e395a8c3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bCGJQ_pgkfl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "pd.set_option(\"display.max_columns\", 50)\n",
        "\n",
        "print(\"Setup OK. Torch version:\", torch.version)\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/LabelCraft_2025/data\"\n",
        "\n",
        "train_path = os.path.join(DATA_DIR, \"labeled_train.parquet\")\n",
        "categories_path = os.path.join(DATA_DIR, \"category_tree.csv\")\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"train_path:\", train_path)\n",
        "print(\"categories_path:\", categories_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка данных и подготовка text_clean"
      ],
      "metadata": {
        "id": "AdEvN_EUhNZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "print(\"Files in DATA_DIR:\", os.listdir(DATA_DIR))\n",
        "\n",
        "train = pd.read_parquet(train_path)\n",
        "cat_tree = pd.read_csv(categories_path)\n",
        "\n",
        "print(\"train:\", train.shape)\n",
        "print(\"category_tree:\", cat_tree.shape)\n",
        "print(\"train columns:\", train.columns.tolist())"
      ],
      "metadata": {
        "id": "BVL0eiPihgqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Склейка текста: source_name + attributes"
      ],
      "metadata": {
        "id": "eWGp08JYhlFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"source_name\"] = train[\"source_name\"].fillna(\"\")\n",
        "train[\"attributes\"] = train[\"attributes\"].fillna(\"\")\n",
        "train[\"text\"] = train[\"source_name\"] + \" \" + train[\"attributes\"]"
      ],
      "metadata": {
        "id": "fbYIzIrEhoRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обрезаем до N слов для baseline"
      ],
      "metadata": {
        "id": "RmFP5EaAhrif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 64\n",
        "\n",
        "def truncate_text(s, max_words=MAX_WORDS):\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    words = s.split()\n",
        "    if len(words) <= max_words:\n",
        "        return s\n",
        "    return \" \".join(words[:max_words])\n",
        "\n",
        "train[\"text_clean\"] = train[\"text\"].apply(truncate_text)\n",
        "train[\"text_clean\"] = train[\"text_clean\"].fillna(\"\").astype(str)"
      ],
      "metadata": {
        "id": "Hp5oL_r7hydT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ограничиваемся подвыборкой и кодируем классы в 0..num_labels-1"
      ],
      "metadata": {
        "id": "CABB6LLAYVSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_SIZE = 40_000 # можно уменьшить до 20_000 при необходимости\n",
        "\n",
        "sample = train.sample(n=SAMPLE_SIZE, random_state=42).copy()"
      ],
      "metadata": {
        "id": "8FJcKTsaYaMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Гарантируем чистый текст"
      ],
      "metadata": {
        "id": "OOtaDJDKbaki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample[\"text_clean\"] = sample[\"text_clean\"].fillna(\"\").astype(str)"
      ],
      "metadata": {
        "id": "Rky4Gofxbde5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Частоты по cat_id в sample"
      ],
      "metadata": {
        "id": "sNcM7vl7bhjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_counts = sample[\"cat_id\"].value_counts()\n",
        "valid_cats = cat_counts[cat_counts >= 2].index"
      ],
      "metadata": {
        "id": "NS8Xv5IwbhGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оставляем только категории с >= 2 объектами"
      ],
      "metadata": {
        "id": "3r9cEHnDbnay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = sample[sample[\"cat_id\"].isin(valid_cats)].copy()"
      ],
      "metadata": {
        "id": "5wKhtYvobg7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим маппинг cat_id -> label_index на отфильтрованных данных"
      ],
      "metadata": {
        "id": "fPe_dmJHYehN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_cats = sorted(sample[\"cat_id\"].unique())\n",
        "cat2label = {cat_id: idx for idx, cat_id in enumerate(unique_cats)}\n",
        "label2cat = {idx: cat_id for cat_id, idx in cat2label.items()}\n",
        "\n",
        "sample[\"label\"] = sample[\"cat_id\"].map(cat2label)\n",
        "\n",
        "print(\"Всего классов после фильтрации:\", len(unique_cats))\n",
        "print(\"Минимальная частота класса:\", sample[\"cat_id\"].value_counts().min())\n",
        "\n",
        "X_text = sample[\"text_clean\"].values\n",
        "y = sample[\"label\"].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train), \"Valid size:\", len(X_valid))"
      ],
      "metadata": {
        "id": "009iSjX8b3oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset и токенизатор"
      ],
      "metadata": {
        "id": "1Ya3FQDvYoPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"cointegrated/rubert-tiny\" # лёгкий русский BERT, можно заменить позже\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "yXxQ0_1lYpCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class ProductsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = list(texts)\n",
        "        self.labels = list(labels)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        item = {key: val.squeeze(0) for key, val in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n"
      ],
      "metadata": {
        "id": "HsnFja3CiXgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ProductsDataset(X_train, y_train, tokenizer, max_length=128)\n",
        "valid_dataset = ProductsDataset(X_valid, y_valid, tokenizer, max_length=128)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Valid dataset size:\", len(valid_dataset))"
      ],
      "metadata": {
        "id": "5UNF614odMBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модель и Trainer"
      ],
      "metadata": {
        "id": "xFhmh75PdTen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(unique_cats)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "MODEL_NAME,\n",
        "num_labels=num_labels\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    macro = f1_score(labels, preds, average=\"macro\")\n",
        "    micro = f1_score(labels, preds, average=\"micro\")\n",
        "\n",
        "    return {\n",
        "    \"macro_f1\": macro,\n",
        "    \"micro_f1\": micro\n",
        "    }"
      ],
      "metadata": {
        "id": "BUPaBYR7dYam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "              output_dir=\"/content/labelcraft_rubert_baseline\",\n",
        "              do_train=True,\n",
        "              do_eval=True,\n",
        "              learning_rate=2e-5,\n",
        "              per_device_train_batch_size=32,\n",
        "              per_device_eval_batch_size=64,\n",
        "              num_train_epochs=1.0,\n",
        "              weight_decay=0.01,\n",
        "              logging_steps=50,\n",
        "              eval_steps=200, # оценка каждые 200 шагов\n",
        "              save_strategy=\"no\", # не сохраняем чекпоинты\n",
        "              report_to=[],\n",
        "              )\n",
        "\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        )\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "OXWenY5kdiP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оценка на валидации"
      ],
      "metadata": {
        "id": "YyQY2YDjdm0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(\"Eval results:\", eval_results)"
      ],
      "metadata": {
        "id": "LJ2dg5wUdp-5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}